{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef25373-1599-442f-bdbf-3c207538ae61",
   "metadata": {},
   "source": [
    "# LMU Code Coffee: [JAX](https://github.com/google/jax)\n",
    "Brett Morris\n",
    "\n",
    "### Install\n",
    "\n",
    "Installation of jax is sometimes [nontrivial](https://github.com/google/jax#installation), but if you're feeling lucky, you can try the following on your laptop (CPU): \n",
    "```bash\n",
    "pip install --upgrade \"jax[cpu]\"\n",
    "```\n",
    "jax can run on GPUs and TPUs but requires specific builds for each system. See the link above for details.\n",
    "\n",
    "### Why jax?\n",
    "\n",
    "It exploits [_autodiff_](https://github.com/hips/autograd) with [_accelerated linear algebra_](https://www.tensorflow.org/xla) with a _numpy-like API_ but a [_just-in-time compiled backend_](https://github.com/google/jax#compilation-with-jit) to calculate blazing fast, differentiable models. Let's break that down: \n",
    "\n",
    "* The automatic differentiation allows you to compute gradients of your mathematical models without explicitly deriving gradients for each function. These gradients can be used for gradient-based inference techniques like Hamiltonian Monte Carlo.\n",
    "* The accelerated linear algebra (XLA) package is an optimizing compiler designed for machine learning. You write Python code and it gets just-in-time compiled before you execute it.\n",
    "\n",
    "\n",
    "### Getting started with jax\n",
    "It looks like ordinary Python code, but you generally use jax's numpyÂ API rather than ordinary numpy to handle array calculations.  \n",
    "\n",
    "For example, here's how we can create an array of linearly spaced values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d13b-f874-4b51-89e0-c63c5d8019aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import numpyro first, though we use it last\n",
    "import numpyro\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro import distributions as dist\n",
    "\n",
    "# Set the number of cores on your machine for parallelism:\n",
    "cpu_cores = 4\n",
    "numpyro.set_host_device_count(cpu_cores)\n",
    "\n",
    "# From jax, we'll import the numpy module as `jnp`\n",
    "from jax import numpy as jnp\n",
    "\n",
    "# Create a linearly spaced `DeviceArray` object, which behaves like a np.ndarray:\n",
    "dev_arr = jnp.linspace(-5, 5, 1_000)\n",
    "\n",
    "dev_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0fae9-f262-41b3-baec-f18f2cf49ff6",
   "metadata": {},
   "source": [
    "Note that the above code created a `DeviceArray` object, which is not an ordinary numpy array. This is limited to data type `float32` by default. `DeviceArray` objects have the usual built-in methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ed652-b9a0-47ff-94c0-4f804608c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_arr.mean(), dev_arr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a69814-1708-46eb-b2e0-491b1243810c",
   "metadata": {},
   "source": [
    "### Creating a synthetic dataset \n",
    "Now let's create some synthetic data which we'll fit using jax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3432e-759b-4fbd-842f-5972c2da24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.array(dev_arr.copy())\n",
    "\n",
    "# Set the parameters of the double-Gaussian \n",
    "# profile in our synthetic data\n",
    "amp0 = 5\n",
    "amp1 = 10\n",
    "x0 = 0.5\n",
    "x1 = -0.2\n",
    "s0 = 1\n",
    "s1 = 0.3\n",
    "yerr = 0.4\n",
    "\n",
    "y = (\n",
    "    amp0 * np.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "    amp1 * np.exp(-0.5 * (x - x1)**2 / s1**2) + \n",
    "    np.random.normal(scale=yerr, size=(len(x)))\n",
    ")\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704ba30-c8e5-4420-b416-83263acecd40",
   "metadata": {},
   "source": [
    "### Fitting with numpy/scipy\n",
    "\n",
    "We could fit the observations $(x, y)$ with numpy and scipy like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8dfbec-27c8-486c-b988-fd805913b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scipy, we'll import the Powell minimizer\n",
    "from scipy.optimize import fmin_powell\n",
    "\n",
    "def model_numpy(p, x):\n",
    "    \"\"\"Numpy implementation of a double-Gaussian profile\"\"\"\n",
    "    a0, x0, s0, a1, x1, s1 = p\n",
    "    return (\n",
    "        a0 * np.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "        a1 * np.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    "    )\n",
    "\n",
    "def chi2_numpy(p, x, y, yerr):\n",
    "    \"\"\"chi^2 function to minimize, using numpy/scipy\"\"\"\n",
    "    return np.sum((model_numpy(p, x) - y)**2 / yerr**2)\n",
    "\n",
    "init_guess = np.array([5, 0, 2, 10, 0, 0.8])\n",
    "\n",
    "bestp_numpy = fmin_powell(chi2_numpy, init_guess, disp=0, args=(x, y, yerr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73d6bb-670a-425a-b72d-ddf48b6e80f1",
   "metadata": {},
   "source": [
    "The `fmin_powell` function does optimization _without_ computing gradients. The best fit solutions are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f52ee-35bd-497e-86d1-3df712de32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestp_numpy   # best fit parameter solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59686e33-033b-4cc2-a71d-a9ba870e061f",
   "metadata": {},
   "source": [
    "Which look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68221083-ffd1-4b91-8a61-bd1ad87ad9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, model_numpy(init_guess, x), 'b')\n",
    "plt.plot(x, model_numpy(bestp_numpy, x), 'r', ls='--', lw=3)\n",
    "plt.gca().set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6c2a5-d3c8-4702-94c8-6fed318363a4",
   "metadata": {},
   "source": [
    "The initial guess is shown above in blue, and the best-fit model using numpy/scipy is shown in red. You can see that the best-fit model doesn't fit the observations particularly well. Now let's implement the same thing in jax. \n",
    "\n",
    "### Fitting with jax\n",
    "\n",
    "Let's specify the model that we will fit to the data using the numpy module within jax. We'll also \"decorate\" it with the `jit` decorator, which will compile the function for us at runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8631f2-0a94-4c20-8423-272c1e70679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the just-in-time decorator\n",
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def model_jax(p, x):     \n",
    "    \"\"\"\n",
    "    Jax implementation of the `model_numpy` function.\n",
    "    \n",
    "    The use of `jnp` in place of `np` is the only difference \n",
    "    from the numpy version.\n",
    "    \"\"\"\n",
    "    a0, x0, s0, a1, x1, s1 = p\n",
    "    return (\n",
    "        a0 * jnp.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "        a1 * jnp.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    "    )\n",
    "@jit\n",
    "def chi2_jax(p, x, y, yerr):\n",
    "    \"\"\"chi^2 function written for minimization with jax\"\"\"\n",
    "    return jnp.sum((model_jax(p, x) - y)**2 / yerr**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699473a2-ba48-4f5e-bfc2-bbe79f556847",
   "metadata": {},
   "source": [
    "Now we import the minimize module from the `scipy.optimize` API within jax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251754d-4d35-4fff-bf6b-64d510234f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax has its own scipy module which uses autodiffed gradients\n",
    "from jax.scipy.optimize import minimize\n",
    "\n",
    "bestp_jax = minimize(chi2_jax, init_guess, args=(x, y, yerr), method='bfgs')\n",
    "\n",
    "# print the best-fit parameters\n",
    "bestp_jax.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55294c8c-5b40-4690-b840-b49ad450124b",
   "metadata": {},
   "source": [
    "In the above cell, we have used _gradient-based_ optimization with the [BFGS method](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Note that we didn't have to specify the gradient of our model with respect to each free parameter, that was done for us!\n",
    "\n",
    "Let's plot the best-fit model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db98d34-fad9-4d66-8b74-8b10a7a719c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'k,')\n",
    "plt.plot(x, model_numpy(init_guess, x), 'b', label='init guess')\n",
    "plt.plot(x, model_numpy(bestp_numpy, x), 'm', ls=':', lw=1.5, label='numpy')\n",
    "plt.plot(x, model_jax(bestp_jax.x, x), 'r', ls='--', lw=3, label='jax')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261553b9-3dd7-4613-926f-bc9724d69600",
   "metadata": {},
   "source": [
    "In the figure above, the blue curve is the initial guess, the magenta dotted curve is the best fit with Powell's method via numpy/scipy, and the red dashed curve is the best-fit with jax. Finally, a good fit!\n",
    "\n",
    "### Speed comparison\n",
    "\n",
    "Now let's check if there's any speed difference between the two implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7298964-6946-4ec1-ab56-02d58133de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numpy only:')\n",
    "time_numpy = %timeit -n 100 -o model_numpy(init_guess, x)\n",
    "print('jax:')\n",
    "time_jax = %timeit -n 100 -o model_jax(init_guess, x)\n",
    "\n",
    "print(f'\\n\\njax model evaluation is {time_numpy.average / time_jax.average :.1f}x faster\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e3617-aecc-4417-afc8-7d91d66e4e7e",
   "metadata": {},
   "source": [
    "So not only is the jax model evaluation is faster, but the best-fit solution is closer to the true answer. Great work jax!\n",
    "\n",
    "### Posterior inference with jax/numpyro\n",
    "\n",
    "Now let's infer posterior distributions for the parameters using more complex inference methods, using _numpyro_. We will define a _model_ which specifies _distributions_ that represent each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd684e3-b41a-43a7-ba03-c2d21dd59799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model():\n",
    "    \"\"\"\n",
    "    Define a model to sample with the No U-Turn Sampler (NUTS) via numpyro.\n",
    "    \n",
    "    The two Gaussians are defined by an amplitude, mean, and standard deviation.\n",
    "    \n",
    "    To find unique solutions for the two Gaussians, we put non-overlapping bounded \n",
    "    priors on the two amplitudes, but vary the means and stddev's with identical \n",
    "    uniform priors. \n",
    "    \"\"\"\n",
    "    # Define priors for non-overlapping Gaussian amplitudes:\n",
    "    a0 = numpyro.sample('amp0', dist.Uniform(low=0, high=8))\n",
    "    a1 = numpyro.sample('amp1', dist.Uniform(low=8, high=30))\n",
    "    \n",
    "    # Uniform priors for means\n",
    "    x0, x1 = numpyro.sample(\n",
    "        'center', dist.Uniform(low=-1, high=1), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "    \n",
    "    # Uniform priors for the stddev's\n",
    "    s0, s1 = numpyro.sample(\n",
    "        'sigma', dist.Uniform(low=0, high=3), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "    \n",
    "    # Normally distributed likelihood\n",
    "    numpyro.sample(\n",
    "        \"obs\", dist.Normal(\n",
    "            loc=model_jax([a0, x0, s0, a1, x1, s1], x), \n",
    "            scale=yerr\n",
    "        ), obs=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d60f0-0ccb-4b91-8180-01275f9f9142",
   "metadata": {},
   "source": [
    "The above cell defines the model. Now the cell below defines how to sample the model, and runs the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc0b59-2c12-422f-9dfb-77c3431e12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random numbers in jax are generated like this:\n",
    "from jax.random import PRNGKey, split\n",
    "\n",
    "rng_seed = 42\n",
    "rng_keys = split(\n",
    "    PRNGKey(rng_seed), \n",
    "    cpu_cores\n",
    ")\n",
    "\n",
    "# Define a sampler, using here the No U-Turn Sampler (NUTS)\n",
    "# with a dense mass matrix:\n",
    "sampler = NUTS(\n",
    "    numpyro_model, \n",
    "    dense_mass=True\n",
    ")\n",
    "\n",
    "# Monte Carlo sampling for a number of steps and parallel chains: \n",
    "mcmc = MCMC(\n",
    "    sampler, \n",
    "    num_warmup=1_000, \n",
    "    num_samples=5_000, \n",
    "    num_chains=cpu_cores\n",
    ")\n",
    "\n",
    "# Run the MCMC\n",
    "mcmc.run(rng_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c9df9-63ad-4bcd-ac2c-acb1d974d16a",
   "metadata": {},
   "source": [
    "Wow, that was fast! Now let's visualize the posteriors using `arviz` and `corner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efcfc2-d68d-4ed4-a782-8094e24a9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these packages will aid in visualization:\n",
    "import arviz\n",
    "from corner import corner\n",
    "\n",
    "# arviz converts a numpyro MCMC object to an `InferenceData` object based on xarray:\n",
    "result = arviz.from_numpyro(mcmc)\n",
    "\n",
    "# these are the inputs to the synthetic double-gaussian profile (blue lines)\n",
    "truths = [amp0, amp1, x0, x1, s0, s1]\n",
    "\n",
    "# make a corner plot\n",
    "corner(\n",
    "    result, \n",
    "    quiet=True, \n",
    "    truths=truths\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a54b4-4ad8-4e68-afe0-c110d0b10e23",
   "metadata": {},
   "source": [
    "Note how all posterior distributions contain the \"true\" value, from which we generated the dataset. We've accurately inferred the six parameters, in no time at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffc91e-148d-493b-9115-f7a1487bf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c698271-cd46-4f8b-8258-19ebe965d81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
