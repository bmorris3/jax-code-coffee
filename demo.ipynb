{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef25373-1599-442f-bdbf-3c207538ae61",
   "metadata": {},
   "source": [
    "# LMU Code Coffee: [JAX](https://github.com/google/jax)\n",
    "Brett Morris\n",
    "\n",
    "### Install\n",
    "\n",
    "Installation of jax is sometimes [nontrivial](https://github.com/google/jax#installation), but if you're feeling lucky, you can try the following on your laptop (CPU): \n",
    "```bash\n",
    "pip install --upgrade \"jax[cpu]\"\n",
    "```\n",
    "jax can run on GPUs and TPUs but requires specific builds for each system. See the link above for details.\n",
    "\n",
    "### Why jax?\n",
    "\n",
    "It exploits [_autodiff_](https://github.com/hips/autograd) with [_accelerated linear algebra_](https://www.tensorflow.org/xla) with a _numpy-like API_ but a [_just-in-time compiled backend_](https://github.com/google/jax#compilation-with-jit) to calculate blazing fast, differentiable models. Let's break that down: \n",
    "\n",
    "* The automatic differentiation allows you to compute gradients of your mathematical models without explicitly deriving gradients for each function. These gradients can be used for gradient-based inference techniques like Hamiltonian Monte Carlo.\n",
    "* The accelerated linear algebra (XLA) package is an optimizing compiler designed for machine learning. You write Python code and it gets just-in-time compiled before you execute it.\n",
    "\n",
    "\n",
    "### What does jax code look like? \n",
    "\n",
    "It looks like ordinary Python code, but you generally use jax's numpyÂ API rather than ordinary numpy to handle array calculations.  \n",
    "\n",
    "For example, here's how we can create an array of linearly spaced values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d13b-f874-4b51-89e0-c63c5d8019aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro import distributions as dist\n",
    "\n",
    "# Set the number of cores on your machine for parallelism:\n",
    "cpu_cores = 4\n",
    "numpyro.set_host_device_count(cpu_cores)\n",
    "\n",
    "from jax import numpy as jnp\n",
    "\n",
    "dev_arr = jnp.linspace(-5, 5, 1_000)\n",
    "\n",
    "dev_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0fae9-f262-41b3-baec-f18f2cf49ff6",
   "metadata": {},
   "source": [
    "Note that the above code created a `DeviceArray` object, which is not an ordinary numpy array. This is limited to data type `float32` by default. `DeviceArray` objects have the usual built-in methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ed652-b9a0-47ff-94c0-4f804608c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_arr.mean(), dev_arr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a69814-1708-46eb-b2e0-491b1243810c",
   "metadata": {},
   "source": [
    "Now let's create some synthetic data which we'll fit using jax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3432e-759b-4fbd-842f-5972c2da24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.array(dev_arr.copy())\n",
    "\n",
    "amp0 = 5\n",
    "amp1 = 10\n",
    "x0 = 0.5\n",
    "x1 = -0.2\n",
    "s0 = 1\n",
    "s1 = 0.3\n",
    "yerr = 0.4\n",
    "\n",
    "y = (\n",
    "    amp0 * np.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "    amp1 * np.exp(-0.5 * (x - x1)**2 / s1**2) + \n",
    "    np.random.normal(scale=yerr, size=(len(x)))\n",
    ")\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.gca().set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704ba30-c8e5-4420-b416-83263acecd40",
   "metadata": {},
   "source": [
    "We could fit the observations $(x, y)$ with numpy and scipy like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8dfbec-27c8-486c-b988-fd805913b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_powell\n",
    "\n",
    "def model_numpy(p, x):\n",
    "    a0, x0, s0, a1, x1, s1 = p\n",
    "    return (\n",
    "        a0 * np.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "        a1 * np.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    "    )\n",
    "\n",
    "def chi2_numpy(p, x, y, yerr):\n",
    "    return np.sum((model_numpy(p, x) - y)**2 / yerr**2)\n",
    "\n",
    "init_guess = np.array([5, 0, 2, 10, 0, 0.8])\n",
    "\n",
    "bestp_numpy = fmin_powell(chi2_numpy, init_guess, disp=0, args=(x, y, yerr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73d6bb-670a-425a-b72d-ddf48b6e80f1",
   "metadata": {},
   "source": [
    "The `fmin_powell` function does optimization _without_ computing gradients. The best fit solutions are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f52ee-35bd-497e-86d1-3df712de32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestp_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59686e33-033b-4cc2-a71d-a9ba870e061f",
   "metadata": {},
   "source": [
    "Which look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68221083-ffd1-4b91-8a61-bd1ad87ad9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, model_numpy(init_guess, x), 'b')\n",
    "plt.plot(x, model_numpy(bestp_numpy, x), 'r', ls='--', lw=3)\n",
    "plt.gca().set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6c2a5-d3c8-4702-94c8-6fed318363a4",
   "metadata": {},
   "source": [
    "Which are all close to their true values. Good job numpy/scipy! Now let's implement the same thing in jax. \n",
    "\n",
    "Let's specify the model that we will fit to the data using the numpy module within jax. We'll also \"decorate\" it with the `jit` decorator, which will compile the function for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8631f2-0a94-4c20-8423-272c1e70679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def model_jax(p, x):     \n",
    "    # the use of `jnp` is the only difference from the numpy version:\n",
    "    a0, x0, s0, a1, x1, s1 = p\n",
    "    return (\n",
    "        a0 * jnp.exp(-0.5 * (x - x0)**2 / s0**2) + \n",
    "        a1 * jnp.exp(-0.5 * (x - x1)**2 / s1**2)\n",
    "    )\n",
    "@jit\n",
    "def chi2_jax(p, x, y, yerr):\n",
    "    return jnp.sum((model_jax(p, x) - y)**2 / yerr**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699473a2-ba48-4f5e-bfc2-bbe79f556847",
   "metadata": {},
   "source": [
    "Now we import the minimize module from the `scipy.optimize` API within jax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251754d-4d35-4fff-bf6b-64d510234f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.optimize import minimize\n",
    "\n",
    "bestp_jax = minimize(chi2_jax, init_guess, args=(x, y, yerr), method='bfgs')\n",
    "\n",
    "bestp_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55294c8c-5b40-4690-b840-b49ad450124b",
   "metadata": {},
   "source": [
    "In the above cell, we have used _gradient-based_ optimization with the [BFGS method](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Note that we didn't have to specify the gradient of our model with respect to each free parameter, that was done for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db98d34-fad9-4d66-8b74-8b10a7a719c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'k,')\n",
    "plt.plot(x, model_numpy(init_guess, x), 'b', label='init guess')\n",
    "plt.plot(x, model_numpy(bestp_numpy, x), 'm', ls=':', lw=1.5, label='numpy')\n",
    "plt.plot(x, model_jax(bestp_jax.x, x), 'r', ls='--', lw=3, label='jax')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261553b9-3dd7-4613-926f-bc9724d69600",
   "metadata": {},
   "source": [
    "Now let's check if there's any speed difference between the two implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7298964-6946-4ec1-ab56-02d58133de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numpy only:')\n",
    "time_numpy = %timeit -n 100 -o model_numpy(init_guess, x)\n",
    "print('jax:')\n",
    "time_jax = %timeit -n 100 -o model_jax(init_guess, x)\n",
    "\n",
    "print(f'\\n\\njax model evaluation is {time_numpy.average / time_jax.average :.1f}x faster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e3617-aecc-4417-afc8-7d91d66e4e7e",
   "metadata": {},
   "source": [
    "So not only is the jax model evaluation is faster, but the best-fit solution is closer to the true answer. Great work jax!\n",
    "\n",
    "Now let's infer posterior distributions for the parameters using more complex inference methods, using _numpyro_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ee0a3-8b7c-49c7-9e84-2ac27ffbd800",
   "metadata": {},
   "source": [
    "We will define a _model_ which specifies _distributions_ that represent each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd684e3-b41a-43a7-ba03-c2d21dd59799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model():\n",
    "    a0 = numpyro.sample('amp0', dist.Uniform(low=0, high=8))\n",
    "    a1 = numpyro.sample('amp1', dist.Uniform(low=8, high=30))\n",
    "    x0, x1 = numpyro.sample(\n",
    "        'center', dist.Uniform(low=-1, high=1), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "    s0, s1 = numpyro.sample(\n",
    "        'sigma', dist.Uniform(low=0, high=3), \n",
    "        sample_shape=(2,)\n",
    "    )\n",
    "    numpyro.sample(\n",
    "        \"obs\", dist.Normal(\n",
    "            loc=model_jax([a0, x0, s0, a1, x1, s1], x), \n",
    "            scale=yerr\n",
    "        ), obs=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc0b59-2c12-422f-9dfb-77c3431e12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey, split\n",
    "\n",
    "rng_seed = 42\n",
    "rng_keys = split(\n",
    "    PRNGKey(rng_seed), \n",
    "    cpu_cores\n",
    ")\n",
    "\n",
    "sampler = NUTS(\n",
    "    numpyro_model, \n",
    "    dense_mass=True\n",
    ")\n",
    "mcmc = MCMC(\n",
    "    sampler, \n",
    "    num_warmup=1_000, \n",
    "    num_samples=5_000, \n",
    "    num_chains=4\n",
    ")\n",
    "\n",
    "mcmc.run(rng_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c9df9-63ad-4bcd-ac2c-acb1d974d16a",
   "metadata": {},
   "source": [
    "Wow, that was fast! Now let's visualize the posteriors using `arviz` and `corner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efcfc2-d68d-4ed4-a782-8094e24a9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz\n",
    "from corner import corner\n",
    "\n",
    "result = arviz.from_numpyro(mcmc)\n",
    "\n",
    "truths = [amp0, amp1, x0, x1, s0, s1]\n",
    "\n",
    "corner(\n",
    "    result, \n",
    "    quiet=True, \n",
    "    truths=truths\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a54b4-4ad8-4e68-afe0-c110d0b10e23",
   "metadata": {},
   "source": [
    "Note how all posterior distributions contain the \"true\" value, from which we generated the dataset. We've accurately inferred the six parameters, in no time at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffc91e-148d-493b-9115-f7a1487bf900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c698271-cd46-4f8b-8258-19ebe965d81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
